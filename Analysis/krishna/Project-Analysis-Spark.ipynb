{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3d469a499803:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1552456532437)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 05:55:41 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@401fbc3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val sc = SparkSession.builder\n",
    "      .appName(\"ddata_analysis\")\n",
    "      .master(\"local[*]\")\n",
    "      .config(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SK_DEVICE_ID: integer (nullable = true)\n",
      " |-- EVENT_DATE: integer (nullable = true)\n",
      " |-- EVENT_TIME: integer (nullable = true)\n",
      " |-- DAYPART_MASK: integer (nullable = true)\n",
      " |-- AD_ID: string (nullable = true)\n",
      " |-- AD_TYPE: string (nullable = true)\n",
      " |-- IGNORE: boolean (nullable = true)\n",
      " |-- CLICKED: boolean (nullable = true)\n",
      " |-- AD_ACTION: string (nullable = true)\n",
      " |-- SK_LOAD_ID: integer (nullable = true)\n",
      "\n",
      "+------------+----------+----------+------------+--------+-------+------+-------+---------+----------+\n",
      "|SK_DEVICE_ID|EVENT_DATE|EVENT_TIME|DAYPART_MASK|   AD_ID|AD_TYPE|IGNORE|CLICKED|AD_ACTION|SK_LOAD_ID|\n",
      "+------------+----------+----------+------------+--------+-------+------+-------+---------+----------+\n",
      "|     5940382|  20180101|     43554|    33686018|14970540|      P| false|  false|         |     61605|\n",
      "|     5866421|  20180101|     43555|    33686018|14970538|      P| false|  false|         |     61593|\n",
      "|     5908830|  20180101|     43555|    33686018|14970538|      P| false|  false|         |     61593|\n",
      "|     5883733|  20180101|     43555|    33686018|14970522|      P| false|  false|         |     61593|\n",
      "|     5883733|  20180101|     43555|    33686018|14970540|      P| false|  false|         |     61593|\n",
      "|     5950553|  20180101|     43555|    33686018|14970522|      P| false|  false|         |     61593|\n",
      "|     5959452|  20180101|     43555|    33686018|14970522|      P| false|  false|         |     61593|\n",
      "|     5964182|  20180101|     43556|    33686018|14969723|      P| false|  false|         |     61593|\n",
      "|     5871183|  20180101|     43556|    33686018|14970538|      P| false|  false|         |     61593|\n",
      "|     5883733|  20180101|     43556|    33686018|14970538|      P| false|  false|         |     61593|\n",
      "+------------+----------+----------+------------+--------+-------+------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ad_Session: org.apache.spark.sql.DataFrame = [SK_DEVICE_ID: int, EVENT_DATE: int ... 8 more fields]\n",
       "Ads_df: org.apache.spark.sql.DataFrame = [SK_DEVICE_ID: int, EVENT_DATE: int ... 8 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Ad_Session=spark.read.option(\"header\",\"true\").parquet(\"hdfs://172.18.0.2:8020/Project/Data/Ad_Session/part-00000-e06ce5ac-fd9c-4465-be71-be8a022f00d0-c000.snappy.parquet\")\n",
    "Ad_Session.printSchema()\n",
    "\n",
    "Ad_Session.createOrReplaceTempView(\"Ads_view\")\n",
    "val Ads_df = spark.sql(\"SELECT * from Ads_view limit 10\")\n",
    "Ads_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CAMPAIGN_ID: string (nullable = true)\n",
      " |-- PLATFORM: string (nullable = true)\n",
      " |-- EFFECTIVE_DATE: timestamp (nullable = true)\n",
      " |-- DESCRIPTION: string (nullable = true)\n",
      " |-- CAMPAIGN: string (nullable = true)\n",
      " |-- ADVERTISER: string (nullable = true)\n",
      " |-- FEED_TYPE: string (nullable = true)\n",
      " |-- ATX_AD_ID: string (nullable = true)\n",
      " |-- EXPIRED_DATE: timestamp (nullable = true)\n",
      " |-- PROGRAM_ID: integer (nullable = true)\n",
      " |-- SOURCE_ID: integer (nullable = true)\n",
      " |-- ORDER_START_DATE: timestamp (nullable = true)\n",
      " |-- ORDER_END_DATE: timestamp (nullable = true)\n",
      " |-- VOD_ASSET: string (nullable = true)\n",
      " |-- BRAND: string (nullable = true)\n",
      " |-- CAMPAIGN_DESCRIPTION: string (nullable = true)\n",
      " |-- AD_TYPE_DESCRIPTION: string (nullable = true)\n",
      " |-- VODAd: string (nullable = true)\n",
      " |-- VODType: string (nullable = true)\n",
      " |-- goto_designator: string (nullable = true)\n",
      " |-- icon_id: integer (nullable = true)\n",
      " |-- MASTER_DEAL_NUMBER: string (nullable = true)\n",
      " |-- DEAL_NUMBER: string (nullable = true)\n",
      " |-- TIME_ZONE: string (nullable = true)\n",
      " |-- REGION_ID: integer (nullable = true)\n",
      "\n",
      "+-----------+-------------------+-------------------+-----------+----------+-------------------+-------------------+------------------+\n",
      "|CAMPAIGN_ID|     EFFECTIVE_DATE|       EXPIRED_DATE|   CAMPAIGN|PROGRAM_ID|   ORDER_START_DATE|     ORDER_END_DATE|MASTER_DEAL_NUMBER|\n",
      "+-----------+-------------------+-------------------+-----------+----------+-------------------+-------------------+------------------+\n",
      "|   14971872|2018-01-01 19:00:00|2018-01-01 19:59:59|TheWall NTL|  14688514|2018-01-01 05:00:00|2018-01-02 04:59:59|              null|\n",
      "|   14977482|2018-01-03 20:00:00|2018-01-03 20:59:59|  9-1-1 ECP|  13308615|2018-01-03 05:00:00|2018-01-04 04:59:59|              null|\n",
      "|   14971238|2018-01-01 19:00:00|2018-01-01 19:59:59|TheWall ECP|  14688514|2018-01-01 05:00:00|2018-01-02 04:59:59|              null|\n",
      "+-----------+-------------------+-------------------+-----------+----------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Campaign: org.apache.spark.sql.DataFrame = [CAMPAIGN_ID: string, PLATFORM: string ... 23 more fields]\n",
       "Campaign_df: org.apache.spark.sql.DataFrame = [CAMPAIGN_ID: string, EFFECTIVE_DATE: timestamp ... 6 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Campaign=spark.read.option(\"header\",\"true\").parquet(\"hdfs://172.18.0.2:8020/Project/Data/Campaign/part-00000-f59398b1-f825-48b0-981b-0d7f9a76bd26-c000.snappy.parquet\")\n",
    "Campaign.printSchema()\n",
    "\n",
    "Campaign.createOrReplaceTempView(\"Campaign_view\")\n",
    "val Campaign_df = spark.sql(\"\"\"SELECT CAMPAIGN_ID,EFFECTIVE_DATE,EXPIRED_DATE,CAMPAIGN,PROGRAM_ID,\n",
    "                            ORDER_START_DATE,ORDER_END_DATE,MASTER_DEAL_NUMBER\n",
    "                            from Campaign_view where CAMPAIGN_ID in ('14977482','14971238','14971872') \"\"\")\n",
    "Campaign_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SK_PROVIDER_ID: integer (nullable = true)\n",
      " |-- SOURCE_ID: integer (nullable = true)\n",
      " |-- SRC_NAME: string (nullable = true)\n",
      " |-- SRC_LONG_NAME: string (nullable = true)\n",
      " |-- SOURCE_TYPE: string (nullable = true)\n",
      " |-- NETWORK: string (nullable = true)\n",
      " |-- HDTV: boolean (nullable = true)\n",
      "\n",
      "+--------------+---------+--------+--------------------+-----------+--------------------+-----+\n",
      "|SK_PROVIDER_ID|SOURCE_ID|SRC_NAME|       SRC_LONG_NAME|SOURCE_TYPE|             NETWORK| HDTV|\n",
      "+--------------+---------+--------+--------------------+-----------+--------------------+-----+\n",
      "|             1|        0|    NULL|                null|       null|                NULL| null|\n",
      "|             1|        1|    NULL|                NULL|       null|                NULL| null|\n",
      "|             1|        2|    NULL|                NULL|       null|                NULL| null|\n",
      "|             1|       88|    NULL|                NULL|       null|                NULL| null|\n",
      "|             1|       99|    NULL|                NULL|       null|                NULL| null|\n",
      "|             1|     4100|   AMC-E|          AMC (East)|      BASIC|                AMC |false|\n",
      "|             1|     4101|   A&E-E|  A&E Network (East)|      BASIC|                 A&E|false|\n",
      "|             1|     4103|   BET-E|Black Entertainme...|      BASIC|Black Entertainme...|false|\n",
      "|             1|     4104| BRAVO-E|        Bravo (East)|      BASIC|              Bravo |false|\n",
      "|             1|     4105| CSPAN-1|Cable Satellite P...|      BASIC|Cable Satellite P...|false|\n",
      "+--------------+---------+--------+--------------------+-----------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Channel: org.apache.spark.sql.DataFrame = [SK_PROVIDER_ID: int, SOURCE_ID: int ... 5 more fields]\n",
       "Channel_df: org.apache.spark.sql.DataFrame = [SK_PROVIDER_ID: int, SOURCE_ID: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Channel=spark.read.option(\"header\",\"true\").parquet(\"hdfs://172.18.0.2:8020/Project/Data/Channel/part-00000-d0fef3d8-a2e4-49d9-bee6-a07e35613c0c-c000.snappy.parquet\")\n",
    "Channel.printSchema()\n",
    "\n",
    "Channel.createOrReplaceTempView(\"Channel_view\")\n",
    "val Channel_df = spark.sql(\"SELECT * from Channel_view limit 10\")\n",
    "Channel_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PROGRAM_ID: string (nullable = true)\n",
      " |-- SUBCATEGORY_ID: string (nullable = true)\n",
      " |-- PROGRAM_SOURCE_ID: string (nullable = true)\n",
      " |-- PROGRAM_AIRING_TYPE_ID: string (nullable = true)\n",
      " |-- CATEGORY_ID: string (nullable = true)\n",
      " |-- CAPTION_ID: string (nullable = true)\n",
      " |-- PROGRAM_COLOR_TYPE_ID: string (nullable = true)\n",
      " |-- MASTER_TITLE: string (nullable = true)\n",
      " |-- RELEASE_YEAR: string (nullable = true)\n",
      " |-- EPISODE_TITLE: string (nullable = true)\n",
      " |-- EPISODE_NUMBER: string (nullable = true)\n",
      " |-- STEREO_ENABLED_YN: string (nullable = true)\n",
      " |-- STAR_RATING: string (nullable = true)\n",
      " |-- SERIES_YN: string (nullable = true)\n",
      " |-- RATING_ID_TV_US: string (nullable = true)\n",
      " |-- RATING_ID_MOVIE_US: string (nullable = true)\n",
      " |-- SERIES_MASTER_YN: string (nullable = true)\n",
      " |-- PARENT_PROGRAM_ID: string (nullable = true)\n",
      " |-- RUNTIME: string (nullable = true)\n",
      " |-- HDTV_YN: string (nullable = true)\n",
      " |-- BRACKET_TEXT: string (nullable = true)\n",
      " |-- PART_NUMBER: string (nullable = true)\n",
      " |-- PART_TOTAL: string (nullable = true)\n",
      " |-- ORIGINAL_ADT: string (nullable = true)\n",
      " |-- SUBTITLE: string (nullable = true)\n",
      " |-- EVENT_DATE: string (nullable = true)\n",
      " |-- TAG: string (nullable = true)\n",
      " |-- Cosmo_program_id: string (nullable = true)\n",
      " |-- link_region_id: string (nullable = true)\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "2019-03-12 16:45:44 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "+----------+-----------------+--------------------+-----------------+--------------+-----------+--------------------+\n",
      "|PROGRAM_ID|PROGRAM_SOURCE_ID|        MASTER_TITLE|     RELEASE_YEAR|EPISODE_NUMBER|STAR_RATING|          EVENT_DATE|\n",
      "+----------+-----------------+--------------------+-----------------+--------------+-----------+--------------------+\n",
      "|   2987362|             5617|         NBA Playoff|             null|          null|       null|2017-04-15 17:30:...|\n",
      "|   5254749|            12011|Partin Family & F...|             null|          null|       null|2019-01-08 04:00:...|\n",
      "|   5254759|            12011|Partin Family & F...|             null|          null|       null|2019-01-09 04:00:...|\n",
      "|   5254798|            12011|Hans Cooks the So...|             null|          null|       null|2019-01-09 19:30:...|\n",
      "|   5254909|            12011|LIVE! GAMETIME: H...|             null|          null|       null|2019-01-18 19:00:...|\n",
      "|   5553053|            25675|     Bien Ã©videmment|             null|          null|       null|2018-01-19 14:48:...|\n",
      "|  12993807|             1048|     WWE TLC: Tables| Ladders & Chairs|          null|          Y|         From Dallas|\n",
      "|   1175846|             2007|      NBA Basketball|             null|          null|       null|2017-01-03 00:00:...|\n",
      "|   2281798|             1185|       PGA Tour Golf|             null|          null|       null|2017-01-05 00:00:...|\n",
      "|   1179661|             2007|      NBA Basketball|             null|          null|       null|2017-02-02 00:00:...|\n",
      "|   1172044|             2220|      NBA Basketball|             null|          null|       null|2017-02-15 00:00:...|\n",
      "|   1159061|            20765|      NBA Basketball|             null|          null|       null|2017-02-28 00:00:...|\n",
      "|   1168294|            11462|      NBA Basketball|             null|          null|       null|2017-03-15 00:00:...|\n",
      "|   1182556|             2301|      NBA Basketball|             null|          null|       null|2017-02-01 00:00:...|\n",
      "|   1151503|            13412|      NBA Basketball|             null|          null|       null|2017-04-06 00:00:...|\n",
      "|   1179663|            11462|      NBA Basketball|             null|          null|       null|2017-03-11 00:00:...|\n",
      "|   4490025|            20735|              Tennis|             null|          null|       null|2017-03-04 00:00:...|\n",
      "|   9540434|            12280|              Tennis|             null|          null|       null|2017-01-07 00:00:...|\n",
      "|   3198274|             1175|        MLL Lacrosse|             null|          null|       null|2017-08-12 00:00:...|\n",
      "|   5233274|            11561|Monster Energy Su...|             null|          null|       null|2017-01-14 00:00:...|\n",
      "+----------+-----------------+--------------------+-----------------+--------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Program: org.apache.spark.sql.DataFrame = [PROGRAM_ID: string, SUBCATEGORY_ID: string ... 29 more fields]\n",
       "Program_df: org.apache.spark.sql.DataFrame = [PROGRAM_ID: string, PROGRAM_SOURCE_ID: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Program=spark.read.option(\"header\",\"true\").parquet(\"hdfs://172.18.0.2:8020/Project/Data/Program/program.parquet\")\n",
    "Program.printSchema()\n",
    "\n",
    "Program.createOrReplaceTempView(\"Program_view\")\n",
    "val Program_df = spark.sql(\"\"\"SELECT PROGRAM_ID,PROGRAM_SOURCE_ID,MASTER_TITLE,RELEASE_YEAR,\n",
    "                            EPISODE_NUMBER,STAR_RATING,EVENT_DATE \n",
    "                            from Program_view where EVENT_DATE > '2016-12-31'\"\"\")\n",
    "Program_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+------------------+\n",
      "|PROGRAM_ID|        MASTER_TITLE|RELEASE_YEAR|       RUNTIME_HRS|\n",
      "+----------+--------------------+------------+------------------+\n",
      "|       188|    High School High|        1996|1.4333333333333333|\n",
      "|       203|The Crow: City of...|        1996|               1.4|\n",
      "|       204|             Tin Cup|        1996| 2.216666666666667|\n",
      "|       245|       The Cable Guy|        1996|1.5833333333333333|\n",
      "|       247| The Nutty Professor|        1996|1.5833333333333333|\n",
      "|       248|             Twister|        1996|1.8833333333333333|\n",
      "|       264|           The Craft|        1996|1.6666666666666667|\n",
      "|       273|               Fargo|        1996|1.6333333333333333|\n",
      "|       477|The Long Kiss Goo...|        1996|               2.0|\n",
      "|       481| The Preacher's Wife|        1996| 2.066666666666667|\n",
      "+----------+--------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT PROGRAM_ID,MASTER_TITLE,RELEASE_YEAR,float(RUNTIME)/3600 as RUNTIME_HRS from Program_view limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+----------------------------+\n",
      "|CAMPAIGN_ID|        MASTER_TITLE|NETWORK|count(DISTINCT SK_DEVICE_ID)|\n",
      "+-----------+--------------------+-------+----------------------------+\n",
      "|   15082593|The Simone Biles ...| NOGGIN|                           5|\n",
      "|   15155317|Unsolved: The Mur...|   NULL|                           3|\n",
      "|   15284281|              Empire|    IND|                           3|\n",
      "|   15036706|        The Resident|    IND|                           2|\n",
      "|   15284926|              Empire|    IND|                           2|\n",
      "|   15036707|        The Resident|    IND|                           1|\n",
      "+-----------+--------------------+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//ads_data : SK_DEVICE_ID|EVENT_DATE|EVENT_TIME|DAYPART_MASK|   AD_ID|AD_TYPE|IGNORE|CLICKED|AD_ACTION|SK_LOAD_ID|\n",
    "//source : SK_PROVIDER_ID|SOURCE_ID|SRC_NAME|       SRC_LONG_NAME|SOURCE_TYPE|             NETWORK| HDTV\n",
    "spark.sql(\"\"\"\n",
    "select CAMPAIGN_ID,MASTER_TITLE,NETWORK,count(distinct SK_DEVICE_ID)\n",
    "from(\n",
    "    select \n",
    "    A.AD_ID,A.SK_DEVICE_ID,A.EVENT_DATE,A.EVENT_TIME,\n",
    "    C.CAMPAIGN,C.CAMPAIGN_ID,C.PROGRAM_ID,C.EFFECTIVE_DATE,C.EXPIRED_DATE,\n",
    "    P.MASTER_TITLE,P.RELEASE_YEAR,\n",
    "    CH.SRC_NAME,CH.NETWORK\n",
    "    from Ads_view A\n",
    "    join Campaign_view C on int(A.AD_ID) = int(C.CAMPAIGN_ID)\n",
    "    join Program_view P on int(C.PROGRAM_ID) = int(P.PROGRAM_ID)\n",
    "    join Channel_view CH on int(P.PROGRAM_SOURCE_ID) = int(CH.SOURCE_ID)\n",
    "    where P.PROGRAM_ID is not null\n",
    ")\n",
    "group by CAMPAIGN_ID,MASTER_TITLE,NETWORK order by 4 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------+------------------+--------------+----------+-------------------+-------------------+---------+------------+------------+\n",
      "|   AD_ID|SK_DEVICE_ID|EVENT_DATE|                ET|      CAMPAIGN|PROGRAM_ID|     EFFECTIVE_DATE|       EXPIRED_DATE|TIME_ZONE|MASTER_TITLE|RELEASE_YEAR|\n",
      "+--------+------------+----------+------------------+--------------+----------+-------------------+-------------------+---------+------------+------------+\n",
      "|14980206|     5865958|  20180104|19.074444444444445|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865958|  20180104|19.057222222222222|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865958|  20180104| 19.05527777777778|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865958|  20180104|19.073055555555555|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865958|  20180104|19.073333333333334|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865958|  20180104| 19.05666666666667|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865958|  20180104|19.056944444444444|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865958|  20180104|19.073888888888888|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865958|  20180104|           19.0575|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104| 19.90138888888889|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104| 19.90222222222222|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865970|  20180104|19.903055555555557|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865970|  20180104|19.825833333333332|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865970|  20180104|19.901944444444446|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104|19.906388888888888|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865970|  20180104| 19.90111111111111|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104|19.903055555555557|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14980206|     5865970|  20180104|19.905555555555555|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104|19.904444444444444|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "|14979948|     5865970|  20180104| 19.82611111111111|Will&Grace ECP|  14688642|2018-01-04 20:00:00|2018-01-04 20:59:59|     null|Will & Grace|        2018|\n",
      "+--------+------------+----------+------------------+--------------+----------+-------------------+-------------------+---------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// AD_ID = '14977482' for show '9-1-1'\n",
    "// AD_ID = '14980206' for show 'will&Grace'\n",
    "//\n",
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from(\n",
    "    select \n",
    "    A.AD_ID,A.SK_DEVICE_ID,A.EVENT_DATE,A.EVENT_TIME/3600 as ET,\n",
    "    C.CAMPAIGN,C.PROGRAM_ID,C.EFFECTIVE_DATE,C.EXPIRED_DATE, C.TIME_ZONE,\n",
    "    P.MASTER_TITLE,P.RELEASE_YEAR\n",
    "    from Ads_view A\n",
    "    join Campaign_view C on int(A.AD_ID) = int(C.CAMPAIGN_ID)\n",
    "    join Program_view P on int(C.PROGRAM_ID) = int(P.PROGRAM_ID)\n",
    "    where P.PROGRAM_ID is not null\n",
    ")\n",
    "where PROGRAM_ID = '14688642' order by 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|CATEGORY_ID|count(1)|\n",
      "+-----------+--------+\n",
      "|          3|   49434|\n",
      "|          8|   61588|\n",
      "|       null|  228398|\n",
      "|          5|  284982|\n",
      "|          9|   12296|\n",
      "|          1|   83685|\n",
      "|          4|   23547|\n",
      "|          2|   75813|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select CATEGORY_ID,count(*)\n",
    "from \n",
    "Program_view \n",
    "group by  CATEGORY_ID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SK_DEVICE_ID: integer (nullable = true)\n",
      " |-- EVENT_DATE: integer (nullable = true)\n",
      " |-- EVENT_TIME: integer (nullable = true)\n",
      " |-- SK_DAYPART_ID: integer (nullable = true)\n",
      " |-- SESSION_ID: integer (nullable = true)\n",
      " |-- EVENT_TYPE: string (nullable = true)\n",
      " |-- SOURCE_ID: integer (nullable = true)\n",
      " |-- PROGRAM_ID: integer (nullable = true)\n",
      " |-- CHANNEL_NUMBER: integer (nullable = true)\n",
      " |-- BACKGROUND_TUNER: boolean (nullable = true)\n",
      " |-- GUIDE_INFLUENCED: boolean (nullable = true)\n",
      " |-- SEARCH_INFLUENCED: boolean (nullable = true)\n",
      " |-- DURATION: integer (nullable = true)\n",
      " |-- PREV_SESSION_ID: integer (nullable = true)\n",
      " |-- PARENT_SESSION_ID: integer (nullable = true)\n",
      " |-- SURFTIME: integer (nullable = true)\n",
      " |-- SK_LOAD_ID: integer (nullable = true)\n",
      " |-- FIRST_USER_ACTION: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Channel_new: org.apache.spark.sql.DataFrame = [SK_DEVICE_ID: int, EVENT_DATE: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Channel_new=spark.read.option(\"header\",\"true\").parquet(\"hdfs://172.18.0.2:8020/Project/Data/Channel_new/part-00000-a1862dd2-8587-416d-9a02-7d66b4f2404d-c000.snappy.parquet\")\n",
    "Channel_new.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 04:30:00 ERROR Executor:91 - Exception in task 3.0 in stage 4.0 (TID 12)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1166)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:805)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:301)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:01 ERROR SparkUncaughtExceptionHandler:91 - Uncaught exception in thread Thread[Executor task launch worker for task 12,5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1166)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:805)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:301)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:01 WARN  TaskSetManager:66 - Lost task 3.0 in stage 4.0 (TID 12, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1166)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:805)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:301)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:256)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2019-03-13 04:30:01 ERROR TaskSetManager:70 - Task 3 in stage 4.0 failed 1 times; aborting job\n",
      "2019-03-13 04:30:01 ERROR TaskSchedulerImpl:91 - Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$4@4ecdd0c9 rejected from java.util.concurrent.ThreadPoolExecutor@79a657af[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 10]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:131)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:488)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:467)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:67)\n",
      "\tat org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-13 04:30:02 ERROR DiskBlockObjectWriter:91 - Uncaught exception while reverting partial writes to file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/17/temp_shuffle_5d1a4fe1-56c1-4ebb-ad1c-7430b8db5a02\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/17/temp_shuffle_5d1a4fe1-56c1-4ebb-ad1c-7430b8db5a02 (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:214)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:105)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:02 ERROR DiskBlockObjectWriter:91 - Uncaught exception while reverting partial writes to file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0c/temp_shuffle_28539ca3-af54-4d8e-8e60-9ea9e55e9a63\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0c/temp_shuffle_28539ca3-af54-4d8e-8e60-9ea9e55e9a63 (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:214)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:105)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:02 ERROR BypassMergeSortShuffleWriter:239 - Error while deleting file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/17/temp_shuffle_5d1a4fe1-56c1-4ebb-ad1c-7430b8db5a02\n",
      "2019-03-13 04:30:02 ERROR BypassMergeSortShuffleWriter:239 - Error while deleting file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0c/temp_shuffle_28539ca3-af54-4d8e-8e60-9ea9e55e9a63\n",
      "2019-03-13 04:30:02 ERROR TaskContextImpl:91 - Error in TaskCompletionListener\n",
      "java.lang.IllegalStateException: Block broadcast_6 not found\n",
      "\tat org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)\n",
      "\tat org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:842)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:265)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:265)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:131)\n",
      "\tat org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)\n",
      "\tat org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:117)\n",
      "\tat org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:130)\n",
      "\tat org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:128)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:128)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:116)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:02 ERROR DiskBlockObjectWriter:91 - Uncaught exception while reverting partial writes to file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0d/temp_shuffle_881dfd70-3f24-4043-915c-d39253cdca8e\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0d/temp_shuffle_881dfd70-3f24-4043-915c-d39253cdca8e (No such file or directory)\n",
      "\tat java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n",
      "\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1369)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:214)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:105)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2019-03-13 04:30:02 ERROR BypassMergeSortShuffleWriter:239 - Error while deleting file /tmp/blockmgr-4668687c-0f47-462e-988c-e6e78d0be5d4/0d/temp_shuffle_881dfd70-3f24-4043-915c-d39253cdca8e\n"
     ]
    }
   ],
   "source": [
    "Channel_new.createOrReplaceTempView(\"Channel_new_view\")\n",
    "val Channel_new_df = spark.sql(\"\"\"SELECT * from Channel_new_view limit 10\"\"\")\n",
    "Channel_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
