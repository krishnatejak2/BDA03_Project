{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"PRO",
				"PROGRAM_ID"
			]
		]
	},
	"buffers":
	[
		{
			"file": "src/main/scala/com/bda/tivo/mthota/ProgramFilter.scala",
			"settings":
			{
				"buffer_size": 6004,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "package com.bda.tivo.krishna\n\nimport com.bda.tivo.utils.commonUtils\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.expressions.Window\n// import org.apache.spark.SparkContext\n// import org.apache.spark.SparkContext._\n// import org.apache.spark.SparkConf\n\nimport org.apache.log4j.{Level, LogManager, PropertyConfigurator}\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\n\nobject Logger{\n  def methodName() = Thread.currentThread().getStackTrace()(3).getMethodName\n  val log = LogManager.getRootLogger\n  val header = \"Your project Description\"\n  def logInfo(msg: String= \"\")= if(msg.length > 0) log.info(String.format(\"(%s) - %s\", methodName(), msg)) else log.info(\"\")\n  def logDebug(msg: String= \"\")= if(msg.length > 0) log.debug(String.format(\"(%s) - %s\", methodName(), msg)) else log.debug(\"\")\n  def logWarn(msg: String= \"\")= if(msg.length > 0) log.warn(String.format(\"(%s) - %s\", methodName(), msg)) else log.warn(\"\")\n  def logError(msg: String= \"\")= if(msg.length > 0) log.error(String.format(\"(%s) - %s\", methodName(), msg)) else log.error(\"\")\n  def title = log.warn(header)\n}\n\n\nobject ChannelPartition {\n\n  def main(args: Array[String]) {\n\n  \tLogger.logInfo(\"Successfully created Spark Configuration Object\")\n\t\n    /********* SPark Config ********/\n    // val conf = new SparkConf().setAppName(\"channel-data-app\")\t//new configuration\n    val conf = new SparkConf().setAppName(\"bda03tivo-channelApp\") //new configuration\n\tval sc = new SparkContext(conf)\t// new SparkContext\n\t\n    Logger.logInfo(\"Successfully created Spark Context\")\n\t\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc) // SQLContext\n    import sqlContext.implicits._ // changes RDD to DF\n    //import org.apache.spark.sql.expressions.Window\n    \n    Logger.logInfo(\"Reading the PROGRAM file...\")\n        val raw_program_data = sqlContext.read.parquet(commonUtils.raw_path_program_data_with_prev_prog)\n        raw_program_data.registerTempTable(\"PROGRAM\")\n        raw_program_data.printSchema()                \n    Logger.logInfo(\"Reading the Program file...Done\")\n    \n    Logger.logInfo(\"Reading the DEVICE file...\")\n        val raw_device_data = sqlContext.read.parquet(commonUtils.raw_path_device_data)\n        raw_device_data.registerTempTable(\"DEVICE\")\n        raw_device_data.printSchema()      \n    Logger.logInfo(\"Reading the DEVICE file...Done\")\n\n    Logger.logInfo(\"Reading the Channel file for All Months...\")\n        val raw_channel_data = sqlContext.read.parquet(commonUtils.raw_path_channel_data)\n        raw_channel_data.registerTempTable(\"WATCH\")\n        raw_channel_data.printSchema()        \n    Logger.logInfo(\"Reading the Channel file for All Months...Done\")    \n\n    Logger.logInfo(\"Joining WATCH and DEVICE info...\")\n    val device_watch_df = sqlContext.sql(\n        \"\"\"\n        select \n            W.*, \n            D.TIME_ZONE, \n            D.SK_SYSTEM_ID\n        from \n            WATCH W\n            left join \n            DEVICE D \n            on W.SK_DEVICE_ID = D.SK_DEVICE_ID\n        \"\"\")\n    Logger.logInfo(\"Joining WATCH and DEVICE info...Done\")\n\n    println( \"Schema of DEVICE_WATCH : After joining DEVICE & WATCH info\" )\n    device_watch_df.printSchema()   \n    device_watch_df.show()\n    // device_watch_df.persist()\n    device_watch_df.registerTempTable(\"DEVICE_WATCH_TABLE\")\n    println(\"Device_Watch Count:\" )\n    println(device_watch_df.count())\n\n    //sqlContext.udf.register(\"add_seconds\", (datetime : java.sql.Timestamp, seconds : Int) => {new java.sql.Timestamp(datetime.getTime() + seconds*1000 )});\n\n    Logger.logInfo(\"Selecting columns from Program...\")\n    val program_select_df = sqlContext.sql(\n        \"\"\"\n            select \n                P.PROGRAM_ID,\n                P.PARENT_PROGRAM_ID,\n                P.MASTER_TITLE,\n                P.CATEGORY_ID,\n                P.SUBCATEGORY_ID,\n                P.RELEASE_YEAR,\n                P.SERIES_MASTER_YN,\n                P.RUNTIME,\n                P.EVENT_DATE,\n                P.EPISODE_NUMBER,\n                P.EPISODE_TITLE,\n                P.ORIGINAL_ADT,\n                P.PROGRAM_EPISODE_RANK,\n                P.PREV_PROGRAM_ID,\n                cast(\n                    case when P.ORIGINAL_ADT in ('NULL') then \n                        (case when P.EVENT_DATE in ('NULL') then null else P.EVENT_DATE end)\n                    else P.ORIGINAL_ADT end as TIMESTAMP\n                ) as PROGRAM_STARTTIME\n            from \n                PROGRAM P \n            where\n                EVENT_DATE <> 'NULL' and EVENT_DATE >= '2018-01-01'\n                \n        \"\"\")\n    Logger.logInfo(\"Selecting columns from Program...Done\")\n   \n    println( \"Schema of PROGRAM Data: After some changes\" )\n    program_select_df.printSchema()      \n    program_select_df.show()\n    // program_select_df.persist()\n    program_select_df.registerTempTable(\"PROGRAM_TABLE\")\n    println( \"Program Count:\" )\n    println(program_select_df.count())\n\n\n    Logger.logInfo(\"Selecting columns from Channel...\")\n    val watch_select_df = sqlContext.sql(\n        \"\"\"\n                select \n                     *, \n                     DATE_FORMAT( CAST( UNIX_TIMESTAMP(cast(EVENT_DATE as string),'yyyyMMdd') as TIMESTAMP ), 'yyyy-MM-dd'  ) as EVENT_DATE_DATEFORMAT,\n                     concat_ws(':', cast(floor(EVENT_TIME/3600) as string), cast (floor((EVENT_TIME % 3600)/60) as string), cast(floor((EVENT_TIME % 60))as string) ) as TIME_CONCAT,\n                     CASE WHEN SURFTIME <= 300 then 1 else 0 end as SURFTIME_VIEWER\n                 from \n                    DEVICE_WATCH_TABLE\n                where\n                    EVENT_DATE >= 20180101 and \n                    EVENT_DATE <= 20181231\n        \"\"\")\n    Logger.logInfo(\"Selecting columns from Channel...Done\")\n\n    println( \"Schema of WATCH Data: After some changes\" )\n    watch_select_df.printSchema()      \n    watch_select_df.show()\n    // watch_select_df.persist()\n    watch_select_df.registerTempTable(\"WATCH_TABLE\")\n    println( \"Watch Count:\" )\n    println(watch_select_df.count())\n\n\n    Logger.logInfo(\"Selecting columns from JOIN...\")\n    val join_data_df = sqlContext.sql(\n        \"\"\"\n        select \n            P.PROGRAM_ID,\n            P.PARENT_PROGRAM_ID,\n            P.MASTER_TITLE,\n            P.CATEGORY_ID,\n            P.SUBCATEGORY_ID,\n            P.RELEASE_YEAR,\n            P.SERIES_MASTER_YN,\n            P.RUNTIME,\n            P.EVENT_DATE,\n            P.EPISODE_NUMBER,\n            P.EPISODE_TITLE,\n            P.PROGRAM_EPISODE_RANK,\n            P.PREV_PROGRAM_ID,\n\n            sum(CH.SURFTIME_VIEWER) as NO_OF_SURF_VIEWERS,\n            -- sum(case when CH.DURATION > 0.7*P.RUNTIME then 1 else 0 end) as NO_OF_FULL_SHOW_VIEWERS,\n            count(distinct CH.SK_DEVICE_ID) as PREV_NO_WATCHERS,\n            avg(CH.SURFTIME) as PREV_AVG_SURF_TIME,\n            avg(CH.DURATION)/3600 as PREV_AVG_DURATION_HRS\n\n        from \n            PROGRAM_TABLE P\n            left join\n            WATCH_TABLE CH\n            on \n            CH.PROGRAM_ID = P.PREV_PROGRAM_ID\n            \n        group by \n            P.PROGRAM_ID,\n            P.PARENT_PROGRAM_ID,\n            P.MASTER_TITLE,\n            P.CATEGORY_ID,\n            P.SUBCATEGORY_ID,\n            P.RELEASE_YEAR,\n            P.SERIES_MASTER_YN,\n            P.RUNTIME,\n            P.EVENT_DATE,\n            P.EPISODE_NUMBER,\n            P.EPISODE_TITLE,\n            P.PROGRAM_EPISODE_RANK,\n            P.PREV_PROGRAM_ID            \n        \"\"\")\n    Logger.logInfo(\"Selecting columns from JOIN...Done\")\n\n    println( \"Schema of FINAL JOIN Data :\" )\n    join_data_df.printSchema()\n    join_data_df.show()\n    // join_data_df.cache()\n    // join_data_df.registerTempTable(\"JOIN_DATA_TABLE\") // register as temptable\n    println( \"JOIN Count:\" )\n    println(join_data_df.count())\n\n\n    join_data_df.repartition(1)\n      .write\n      .mode(saveMode = \"Overwrite\")\n      .option(\"header\", \"true\")\n      .parquet(commonUtils.partitioned_path_channel_data+\"parquet/\")\n    //read_data.write.format.csv(\"ads.csv\")\n    //read_data.write.format(\"com.databricks.spark.csv\").save(\"/user/bda03tivo/ads_feb_test/mydata.csv\")\n    Logger.logInfo(\"Stopping SparkContext...\")\n    sc.stop()\n    Logger.logInfo(\"Stopping SparkContext...Done\")\n  }\n}\n\n",
			"file": "src/main/scala/com/bda/tivo/krishna/ChannelPartition.scala",
			"file_size": 8340,
			"file_write_time": 132032514912119888,
			"settings":
			{
				"buffer_size": 8358,
				"line_ending": "Unix"
			}
		},
		{
			"file": "build.sbt",
			"settings":
			{
				"buffer_size": 331,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "src/main/scala/com/bda/tivo/utils/commonUtils.scala",
			"settings":
			{
				"buffer_size": 6944,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"console":
	{
		"height": 0.0,
		"history":
		[
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/Users/krishna/Documents/GitHub/BDA03_Project/IIM",
		"/Users/krishna/Documents/GitHub/BDA03_Project/IIM/project"
	],
	"file_history":
	[
		"/Users/krishna/BDA03/Docker-setup/Docker_Commands",
		"/Users/krishna/BDA03/Docker-compose-commands",
		"/Users/krishna/BDA03/Final-Project/Discussion_text",
		"/Users/krishna/Documents/GitHub/BDA03_Project/IIM/src/main/scala/com/bda/tivo/krishna/SimpleApp.scala",
		"/Users/krishna/Documents/GitHub/BDA03_Project/IIM/build.sbt",
		"/Users/krishna/BDA03/Docker-setup/docker-compose.yml",
		"/Users/krishna/BDA03/Docker_Commands",
		"/Users/krishna/BDA03/CDH-Docker/Commands",
		"/Users/krishna/BDA03/pandoc",
		"/Users/krishna/BDA03/Class_Notes",
		"/Users/krishna/BDA03/Module5/Commands-Mock",
		"/Users/krishna/BDA03/MOOC's/Docker-Training",
		"/Users/krishna/Downloads/Read_Files_Identify_Keywords.py",
		"/Users/krishna/BDA03/To Do - Dec 13",
		"/Users/krishna/BDA03/Docker-Training",
		"/Users/krishna/BDA03/Read_Files_Identify_Keywords.py",
		"/Users/krishna/Codes/Python-Codes/python_test/Test2.txt",
		"/Users/krishna/Codes/Python-Codes/python_test/Test1.txt",
		"/Users/krishna/BDA03/Module5/reducer.py",
		"/Users/krishna/BDA03/Module5/mapper.py"
	],
	"find":
	{
		"height": 39.0
	},
	"find_in_files":
	{
		"height": 101.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			" \\",
			"  ",
			"-p",
			"\""
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			""
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 3,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "src/main/scala/com/bda/tivo/mthota/ProgramFilter.scala",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6004,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Scala/Scala.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "src/main/scala/com/bda/tivo/krishna/ChannelPartition.scala",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8358,
						"regions":
						{
						},
						"selection":
						[
							[
								4268,
								4268
							]
						],
						"settings":
						{
							"syntax": "Packages/Scala/Scala.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1128.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "build.sbt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 331,
						"regions":
						{
						},
						"selection":
						[
							[
								206,
								218
							]
						],
						"settings":
						{
							"syntax": "Packages/Scala/Scala.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "src/main/scala/com/bda/tivo/utils/commonUtils.scala",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6944,
						"regions":
						{
						},
						"selection":
						[
							[
								1286,
								1286
							]
						],
						"settings":
						{
							"syntax": "Packages/Scala/Scala.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 24.0
	},
	"input":
	{
		"height": 0.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "IIM-project.sublime-project",
	"replace":
	{
		"height": 70.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 394.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
